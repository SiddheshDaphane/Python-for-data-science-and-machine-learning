{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)\n",
    "\n",
    "## What is K-Nearest Neighbors?\n",
    "K-Nearest Neighbors (KNN) is a **supervised machine learning algorithm** used for both **classification** and **regression** tasks. It is a **non-parametric** and **instance-based** learning method, meaning it does not make assumptions about the underlying data distribution and memorizes the training data to make predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## Why is KNN Used?\n",
    "- **For Simplicity**: KNN is easy to implement and intuitive.\n",
    "- **Versatility**: It can handle classification and regression problems.\n",
    "- **Non-parametric**: Useful for datasets where no assumption about the data distribution can be made.\n",
    "- **Adaptable**: Works well with multi-class classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "## How Does KNN Work?\n",
    "\n",
    "### 1. Data Representation\n",
    "- Each data point is represented in an n-dimensional feature space.\n",
    "- For classification, each point is labeled with a class.\n",
    "- For regression, each point has a continuous target value.\n",
    "\n",
    "### 2. Prediction\n",
    "When a new data point needs to be classified or predicted, the algorithm:\n",
    "1. Calculates the **distance** (e.g., Euclidean distance) from the new point to all training points.\n",
    "2. Selects the **k nearest neighbors** (smallest distances).\n",
    "3. Aggregates the neighbors:\n",
    "   - **Classification**: Assigns the most common class (majority voting).\n",
    "   - **Regression**: Calculates the average (or weighted average) of their target values.\n",
    "\n",
    "### 3. Distance Metrics\n",
    "Commonly used metrics for calculating distances:\n",
    "- **Euclidean Distance**: sqrt(sum((x_i - y_i)^2))\n",
    "- **Manhattan Distance**: sum(|x_i - y_i|)\n",
    "- **Minkowski Distance**: Generalized form combining Euclidean and Manhattan distances.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Technical Terms in KNN\n",
    "- **k (Number of Neighbors)**:\n",
    "  - A hyperparameter that determines how many neighbors to consider.\n",
    "  - Small k: Sensitive to noise (overfitting).\n",
    "  - Large k: Can underfit by ignoring local patterns.\n",
    "- **Weighted Voting**:\n",
    "  - Neighbors closer to the new point may have higher influence.\n",
    "- **Instance-based Learning**:\n",
    "  - Unlike parametric algorithms (e.g., Logistic Regression), KNN doesn’t build a model. It memorizes the training data.\n",
    "- **Lazy Learning**:\n",
    "  - The algorithm does no computation during training but performs all the work during prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## How is KNN Different from Logistic Regression?\n",
    "\n",
    "| **Feature**                 | **KNN**                                     | **Logistic Regression**                     |\n",
    "|-----------------------------|---------------------------------------------|---------------------------------------------|\n",
    "| **Type**                    | Non-parametric                             | Parametric (assumes linear decision boundary). |\n",
    "| **Learning Approach**       | Instance-based (lazy)                      | Model-based (eager).                        |\n",
    "| **Data Assumptions**        | No assumptions about the data distribution.| Assumes a linear relationship between features and log-odds. |\n",
    "| **Complexity**              | Simple to implement, computationally expensive during prediction.| Requires training, computationally faster during prediction. |\n",
    "| **Performance on Large Data** | Struggles with very large datasets.       | Scales better for large datasets.           |\n",
    "| **Multicollinearity**       | Unaffected by multicollinearity.            | Sensitive to multicollinearity.             |\n",
    "\n",
    "---\n",
    "\n",
    "## When to Use KNN?\n",
    "- **Low-dimensional datasets**: KNN struggles with high-dimensional data (curse of dimensionality).\n",
    "- **Balanced datasets**: Works well when classes are evenly distributed.\n",
    "- **Non-linear decision boundaries**: Logistic regression may fail where KNN can adapt to complex patterns.\n",
    "- **Small datasets**: Since it stores all training data, KNN is memory-intensive for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## How to Evaluate KNN?\n",
    "\n",
    "### Classification Metrics\n",
    "- **Accuracy**: Proportion of correctly classified samples.\n",
    "- **Precision, Recall, F1-Score**: Handle imbalanced datasets effectively.\n",
    "- **Confusion Matrix**: Provides a breakdown of TP, TN, FP, and FN.\n",
    "- **ROC-AUC Score**: For evaluating the model's ability to distinguish between classes.\n",
    "\n",
    "### Regression Metrics\n",
    "- **Mean Absolute Error (MAE)**.\n",
    "- **Mean Squared Error (MSE)**.\n",
    "- **R² Score**: Measures how well the regression predictions approximate the true values.\n",
    "\n",
    "### Cross-validation\n",
    "- Splits data into train/test sets multiple times to evaluate performance robustly.\n",
    "\n",
    "### Grid Search for k\n",
    "- Use grid search with cross-validation to determine the optimal k value.\n",
    "\n",
    "---\n",
    "\n",
    "## Strengths of KNN\n",
    "- **Simple and Easy to Implement**.\n",
    "- **No Training Phase**: Computationally cheap during training.\n",
    "- **Non-linear Boundaries**: Can adapt to complex decision boundaries.\n",
    "- **Robust to Noise** (if k is chosen carefully).\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations of KNN\n",
    "- **Computational Cost**: Prediction requires computing distances to all training samples, which can be slow for large datasets.\n",
    "- **Curse of Dimensionality**: In high-dimensional spaces, distances become less meaningful.\n",
    "- **Sensitive to Scaling**: Features need to be standardized or normalized for fair distance calculation.\n",
    "- **Choice of k**: Selecting an optimal k value is crucial and can vary based on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- KNN is a simple, versatile algorithm suitable for small, non-linear, and low-dimensional datasets.\n",
    "- It differs from Logistic Regression by being non-parametric and instance-based.\n",
    "- The algorithm’s performance depends heavily on:\n",
    "  - The choice of distance metric.\n",
    "  - The number of neighbors (k).\n",
    "  - Proper scaling of features.\n",
    "- Evaluation involves classification and regression metrics, depending on the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13 (main, Sep 11 2023, 08:16:02) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e62015e6b186da99cb8ad678637cceaa2b225465cd54bda86f729cbc8f9c829"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
